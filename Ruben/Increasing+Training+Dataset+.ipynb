{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increasing training dataset for NLP Tasks\n",
    "##### By Ruben Seoane\n",
    "\n",
    "**Brief:**\n",
    "By using NLG solutions (backtranslation, paraphrasing, article spinning) provided for free, we expect to be able to generate additional text snippets from the existing dataset, thus avoiding the need for human labeling and providing the DL model with data that maintains the conceptual relationships between entities but not necessarily the same semantic structure, hypothesizing that this approach will help for a better understanding of relationships, based on higher level meaning and not depending on specific words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6babc007516b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import training set and dropping existing duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Import training set and dropping existing duplicates\n",
    "dataset = pd.read_csv(\"train.csv\")\n",
    "dataset.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We observed that the training data contained several instances where different words including company names were concatenated together, so we implemented the following_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding spaces between words and entities\n",
    "for i in range(dataset.shape[0]):\n",
    "    current_row = dataset.iloc[i]\n",
    "    for company in companies:\n",
    "        current_row[\"snippet\"].replace(company, ' ' + company +' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Paraphrasing needs only to be applied on the forth column \"snippet\":_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create new file with ´snippet´ column only\n",
    "keep_col = ['snippet']\n",
    "snippet_col = dataset[keep_col]\n",
    "snippet_col.to_csv('snippet_only_original.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_After trying to implement several \"article spinning\" APIs, and trying to set up a function to perform \"backtranslation\" (English-->French, French-->English...) to generate semantic variations, I found the service \"https://www.prepostseo.com/free-online-article-rewriter\" to be an easier approach. However, after testing with increasing text files, the service limit of 10Mb required the modified dataset (17Mb) to be split into two_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   A- Transforming CSV into a list\n",
    "with open('snippet_only_original.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    snippet_list = list(reader)\n",
    "    \n",
    "#   B- Dividing list into two\n",
    "snippet_l_one = snippet_list[:40000]\n",
    "snippet_l_two = snippet_list[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export lists as CSV file for rewriting process\n",
    "df = pd.DataFrame(snippet_l_one, columns=[\"column\"])\n",
    "df.to_csv('snippet_A.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_two, columns=[\"column\"])\n",
    "df.to_csv('snippet_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I decided to export the above as CSV instead of directly to .TXT in order to review the results and use Excel to manually export into Tab Delimited file and Unicode text, as differences as delimiters by tab, space or \" \" were bringing different results from the text spinning tool._\n",
    "\n",
    "_At this point, the generated .txt files, \"TAB_text_snippet_A.txt\" and \"TAB_text_snippet_A.txt\" were sent to the above mentioned site resulting in the navigator (Chrome and Firefox) crashing multiple times. In previous attemps, uploading the first 1000 rows (=1000 sentences or paragraphs) required 100-110 minutes of processing, which means that if the load of this process increases linearly, it could require around 8,800 minutes, or ~147 hours. Given the limited amount of time, and that we haven't been able to access any cloud provider to test if this task could be accelerated, I'll approach this task focusing in the 30% of the dataset that has True \"Is Parent\" relationships, to level the proportion between negative or neutral text snippets and the ones containing true ownership relationships._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   B2- Slicing list into 10 parts - After failed attempt, previous upload and processing of data through the text reqriting service proved to be extremely slow\n",
    "snippet_l_1 = snippet_list[:8000]\n",
    "snippet_l_2 = snippet_list[8000:16000]\n",
    "snippet_l_3 = snippet_list[16000:32000]\n",
    "snippet_l_4 = snippet_list[32000:38000]\n",
    "snippet_l_5 = snippet_list[38000:44000]\n",
    "snippet_l_6 = snippet_list[44000:50000]\n",
    "snippet_l_7 = snippet_list[50000:58000]\n",
    "snippet_l_8 = snippet_list[58000:64000]\n",
    "snippet_l_9 = snippet_list[64000:70000]\n",
    "snippet_l_10 = snippet_list[70000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export lists as CSV file for rewriting process\n",
    "df = pd.DataFrame(snippet_l_1, columns=[\"column\"])\n",
    "df.to_csv('snippet_1.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_2, columns=[\"column\"])\n",
    "df.to_csv('snippet_2.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_3, columns=[\"column\"])\n",
    "df.to_csv('snippet_3.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_4, columns=[\"column\"])\n",
    "df.to_csv('snippet_4.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_5, columns=[\"column\"])\n",
    "df.to_csv('snippet_5.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_6, columns=[\"column\"])\n",
    "df.to_csv('snippet_6.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_7, columns=[\"column\"])\n",
    "df.to_csv('snippet_7.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_8, columns=[\"column\"])\n",
    "df.to_csv('snippet_8.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_9, columns=[\"column\"])\n",
    "df.to_csv('snippet_9.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_10, columns=[\"column\"])\n",
    "df.to_csv('snippet_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abandoning approach:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_After generating the above 10 samples (~8,000 rows of sentences/paragraphs each), processing the first text is still at 11% after almost 3 hours running through https://www.prepostseo.com/free-online-article-rewriter. This we have decided to stop attempting this approach, as there is virtually no time to have the extra data ready to train on before the deadline. If we were to attempt this approach with more time, one of the following approaches will be best fitted:_ \\n *Create own function with call to Google API to perform backtranslation. \\n *Build own NLG model to rewrite text. \\n _This models could be executed on the cloud or a GPU system and be more controllable as well as execute faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future reference:**\n",
    "_There are multiple providers of corporate databases that encompass houndreds of millions of companies globally, accounting for their estructure, financials etc. Mapping parent-child relationships could be done on them (they offer demos or free-trial access). For each pair were a true relationship exist, a script could be executed, which will input \"Company1\" AND \"company2\" through Google Search API, and could gather news articles were both companies appear together, even extending the search through time (in order to incorporate multiple articles where both appear and potentially their ownership relationship is expressed in some manner). The main body of this articles could be extracted or reduced programatically to a paragraph containing the two entities, and use it to build the database. Examples of this databases are: LexisNexis, Westlaw, Factiva and ProQuest._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
