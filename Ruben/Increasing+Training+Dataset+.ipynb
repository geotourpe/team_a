{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increasing training dataset for NLP Tasks\n",
    "##### By Ruben Seoane\n",
    "\n",
    "**Brief:**\n",
    "By using NLG solutions (backtranslation, paraphrasing, article spinning) provided for free, we expect to be able to generate additional text snippets from the existing dataset, thus avoiding the need for human labeling and providing the DL model with data that maintains the conceptual relationships between entities but not necessarily the same semantic structure, hypothesizing that this approach will help for a better understanding of relationships, based on higher level meaning and not depending on specific words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Import training set and dropping existing duplicates\n",
    "dataset = pd.read_csv(\"train.csv\")\n",
    "dataset.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We observed that the training data contained several instances where different words including company names were concatenated together, so we implemented the following_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding spaces between words and entities\n",
    "for i in range(dataset.shape[0]):\n",
    "    current_row = dataset.iloc[i]\n",
    "    for company in companies:\n",
    "        current_row[\"snippet\"].replace(company, ' ' + company +' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Paraphrasing needs only to be applied on the forth column \"snippet\":_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create new file with ´snippet´ column only\n",
    "keep_col = ['snippet']\n",
    "snippet_col = dataset[keep_col]\n",
    "snippet_col.to_csv('snippet_only_original.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_After trying to implement several \"article spinning\" APIs, and trying to set up a function to perform \"backtranslation\" (English-->French, French-->English...) to generate semantic variations, I found the service \"https://www.prepostseo.com/free-online-article-rewriter\" to be an easier approach. However, after testing with increasing text files, the service limit of 10Mb required the modified dataset (17Mb) to be split into two_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   A- Transforming CSV into a list\n",
    "with open('snippet_only_original.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    snippet_list = list(reader)\n",
    "    \n",
    "#   B- Dividing list into two\n",
    "snippet_l_one = snippet_list[:40000]\n",
    "snippet_l_two = snippet_list[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export lists as CSV file for rewriting process\n",
    "df = pd.DataFrame(snippet_l_one, columns=[\"column\"])\n",
    "df.to_csv('snippet_A.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_two, columns=[\"column\"])\n",
    "df.to_csv('snippet_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I decided to export the above as CSV instead of directly to .TXT in order to review the results and use Excel to manually export into Tab Delimited file and Unicode text, as differences as delimiters by tab, space or \" \" were bringing different results from the text spinning tool.\n",
    "\n",
    "At this point, the generated .txt files, \"TAB_text_snippet_A.txt\" and \"TAB_text_snippet_A.txt\" were sent to the above mentioned site resulting in the navigator (Chrome and Firefox) crashing multiple times. In previous attemps, uploading the first 1000 rows (=1000 sentences or paragraphs) required 100-110 minutes of processing, which means that if the load of this process increases linearly, it could require around 8,800 minutes, or ~147 hours. Given the limited amount of time, and that we haven't been able to access any cloud provider to test if this task could be accelerated, I'll approach this task focusing in the 30% of the dataset that has True \"Is Parent\" relationships, to level the proportion between negative or neutral text snippets and the ones containing true ownership relationships._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   B2- Slicing list into 10 parts - After failed attempt, previous upload and processing of data through the text reqriting service proved to be extremely slow\n",
    "snippet_l_1 = snippet_list[:8000]\n",
    "snippet_l_2 = snippet_list[8000:16000]\n",
    "snippet_l_3 = snippet_list[16000:32000]\n",
    "snippet_l_4 = snippet_list[32000:38000]\n",
    "snippet_l_5 = snippet_list[38000:44000]\n",
    "snippet_l_6 = snippet_list[44000:50000]\n",
    "snippet_l_7 = snippet_list[50000:58000]\n",
    "snippet_l_8 = snippet_list[58000:64000]\n",
    "snippet_l_9 = snippet_list[64000:70000]\n",
    "snippet_l_10 = snippet_list[70000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export lists as CSV file for rewriting process\n",
    "df = pd.DataFrame(snippet_l_1, columns=[\"column\"])\n",
    "df.to_csv('snippet_1.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_2, columns=[\"column\"])\n",
    "df.to_csv('snippet_2.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_3, columns=[\"column\"])\n",
    "df.to_csv('snippet_3.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_4, columns=[\"column\"])\n",
    "df.to_csv('snippet_4.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_5, columns=[\"column\"])\n",
    "df.to_csv('snippet_5.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_6, columns=[\"column\"])\n",
    "df.to_csv('snippet_6.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_7, columns=[\"column\"])\n",
    "df.to_csv('snippet_7.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_8, columns=[\"column\"])\n",
    "df.to_csv('snippet_8.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_9, columns=[\"column\"])\n",
    "df.to_csv('snippet_9.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(snippet_l_10, columns=[\"column\"])\n",
    "df.to_csv('snippet_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abandoning approach:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_After generating the above 10 samples (~8,000 rows of sentences/paragraphs each), processing the first file is still at 11% after almost 3 hours running through https://www.prepostseo.com/free-online-article-rewriter. Thus we have decided to halt this approach, as there is virtually no time to have the extra data ready to train on before the deadline. If we were to attempt this methodology with more time, one of the following approaches will be best fitted:_ <br> - Create own function with call to Google API to perform backtranslation. <br> - Build own NLG model to rewrite text. <br> _This models could be executed on the cloud or a GPU system and be more controllable as well as having faster performance._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future reference:**<br>\n",
    "_There are multiple providers of corporate databases that encompass hundreds of millions of companies globally, accounting for their estructure, financials etc. Mapping parent-child relationships could be done on them (they offer demos or free-trial access). For each pair where a true relationship exist, a script could be executed, which will input \"Company1\" AND \"Company2\" through Google Search API, and could gather news articles were both companies appear together, even extending the search through time (in order to incorporate multiple articles where both appear and potentially their ownership relationship is expressed in some manner). The main body of these articles could be extracted or reduced programatically to a paragraph containing the two entities, and use it to build the database. Examples of this databases are: LexisNexis, Westlaw, Factiva and ProQuest._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
