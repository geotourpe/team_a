{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This module contains the class SentenceProcessing which consists of functions useful for pre-processing the sentences to desired lengths.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from CustomExceptions.CustomExceptions import SentenceLongerError, SentenceShorterError, DocumentLongerError, DocumentShorterError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceProcessing(object):\n",
    "        \n",
    "    def merge_small_sentences(self, tokenized_sentences_tokenized_words, min_len = 3):\n",
    "        '''\n",
    "        This function merges small sentences with previous sentences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokenized_sentences_tokenized_words : list of lists\n",
    "            Each inner list consists of tokenized words corresponding to a sentence.\n",
    "        min_len : int\n",
    "            The minimum threshold length of a sentence.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reformatted_tokenized_sentences_tokenized_words : list of lists\n",
    "            Each inner list will be of minimum length.\n",
    "\n",
    "        '''\n",
    "    \n",
    "        reformatted_tokenized_sentences_tokenized_words = [[]]\n",
    "        for sent in tokenized_sentences_tokenized_words:\n",
    "            if len(sent)>min_len:\n",
    "                reformatted_tokenized_sentences_tokenized_words.append(sent)\n",
    "            else:\n",
    "                reformatted_tokenized_sentences_tokenized_words[-1] = reformatted_tokenized_sentences_tokenized_words[-1] + sent\n",
    "\n",
    "        if reformatted_tokenized_sentences_tokenized_words[0] == []:\n",
    "            del reformatted_tokenized_sentences_tokenized_words[0]\n",
    "        return reformatted_tokenized_sentences_tokenized_words\n",
    "    \n",
    "    def truncate_sent(self, sent, chosen_sent_len):\n",
    "        '''\n",
    "        This function truncates a sentence longer than chosen_sent_len into lists of length ``chosen_sent_len``.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sent : list\n",
    "            A list of words\n",
    "        chosen_sent_len : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        reformatted_sent : list of lists\n",
    "            Each inner list is the truncated input list ``sent``.        \n",
    "        \n",
    "        '''\n",
    "         \n",
    "        if len(sent) >= chosen_sent_len:\n",
    "            if len(sent)%chosen_sent_len == 0:\n",
    "                reformatted_sent = [sent[x*chosen_sent_len:(x*chosen_sent_len)+chosen_sent_len] for x in range(int(len(sent)/chosen_sent_len))]\n",
    "            else:\n",
    "                reformatted_sent = [sent[x*chosen_sent_len:(x*chosen_sent_len)+chosen_sent_len] for x in range(int(len(sent)/chosen_sent_len) + 1 )]\n",
    "            return reformatted_sent\n",
    "        else:\n",
    "            raise SentenceShorterError\n",
    "    \n",
    "    def pad_sent(self, sent, chosen_sent_len, dummy_token = 'my_dummy'):\n",
    "        '''\n",
    "        This function pads a dummy token - my_dummy to sentences shoerter than chosen_sent_len.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sent : list\n",
    "            A list of words\n",
    "        chosen_sent_len : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        reformatted_sent : list\n",
    "            The input list is padded with dummy token.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if len(sent) <= chosen_sent_len:\n",
    "            reformatted_sent = sent + [dummy_token] * (chosen_sent_len - len(sent))\n",
    "            return reformatted_sent\n",
    "        else:\n",
    "            raise SentenceLongerError\n",
    "    \n",
    "    def pad_truncate_sent(self, tokenized_sentences_tokenized_words, chosen_sent_len, dummy_token='my_dummy'):\n",
    "        '''\n",
    "        This function pads/truncates a list of tokenized sentences.\n",
    "        First, if the length of the sentence is longer than the chosen_sent_len, then its truncated into sublists.\n",
    "        Else, if the length of the sentence is shorter than the chosen_sent_len, then its padded with dummy tokens.\n",
    "        Else, if the length of the sentence is equal to the chosen_sent_len, then nothing is done.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tokenized_sentences_tokenized_words : List of list\n",
    "            Each inner list consists of tokenized words corresponding to a sentence.\n",
    "        \n",
    "        chosen_sent_len : int\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        padded_truncated_tokenized_sentences_tokenized_words : List of lists \n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        truncate_sent function returns a list of lists as output whereas pad_sent function returns only a list asd output.\n",
    "        \n",
    "        '''\n",
    "        padded_truncated_tokenized_sentences_tokenized_words = []\n",
    "        for sent in tokenized_sentences_tokenized_words:\n",
    "            if len(sent) > chosen_sent_len:\n",
    "                for truncated_sentences in self.truncate_sent(sent, chosen_sent_len):\n",
    "                    padded_truncated_tokenized_sentences_tokenized_words.append(self.pad_sent(truncated_sentences, chosen_sent_len, dummy_token))\n",
    "            if len(sent) < chosen_sent_len:\n",
    "                    padded_truncated_tokenized_sentences_tokenized_words.append(self.pad_sent(sent, chosen_sent_len, dummy_token))\n",
    "            if len(sent) == chosen_sent_len:\n",
    "                padded_truncated_tokenized_sentences_tokenized_words.append(sent)\n",
    "        return padded_truncated_tokenized_sentences_tokenized_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DartsProcessing",
   "language": "python",
   "name": "dartsprocessing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
