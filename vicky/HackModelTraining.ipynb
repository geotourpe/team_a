{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_processing_scripts_path = os.path.abspath('TextProcessingScripts')\n",
    "sys.path.append(text_processing_scripts_path)\n",
    "\n",
    "scripts_path = os.path.abspath('Scripts')\n",
    "sys.path.append(scripts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from CustomNN import create_RNN, create_attention\n",
    "from CustomRNN import CustomRNN\n",
    "from LengthEstimation import estimate_sentences_and_document_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from CommonUtilities.FileUtilities import return_file_content, save_pickle_file, load_pickle_file\n",
    "from CreateTrainingBatches import CreateTrainingBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.abspath('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = load_pickle_file(os.path.join(data_path, 'training_data.p'))\n",
    "validation_data = load_pickle_file(os.path.join(data_path, 'validation_data.p'))\n",
    "training_params = load_pickle_file(os.path.join(data_path, 'training_params.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = training_params['embedding_matrix']\n",
    "rev_vocab_dict = training_params['rev_vocab_dict']\n",
    "\n",
    "estimated_doc_len = training_params['estimated_doc_len']\n",
    "estimated_sent_len = training_params['estimated_sent_len']\n",
    "\n",
    "vocab_dict = training_params['vocab_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_training_batches = CreateTrainingBatches(training_data['X_train'], training_data['y_train'],\n",
    "                                                validation_data['X_valid'], validation_data['y_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_neurons_GRU_1 = 50\n",
    "n_neurons_GRU_2 = 100\n",
    "attention_n_neurons_1 = 100\n",
    "attention_n_neurons_2 = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.name_scope('Inputs'):\n",
    "        X = tf.placeholder(tf.int32, [None, estimated_doc_len, estimated_sent_len], name='X')\n",
    "        y = tf.placeholder(tf.float32, [None, 1], name='y')\n",
    "        tf_sentences_length = tf.placeholder(tf.int32, [None], name = 'sentences_length')\n",
    "        tf_documents_length = tf.placeholder(tf.int32, [None], name = 'documents_length')\n",
    "        tf_keep_prob = tf.placeholder(tf.float32, name='tf_keep_prob')\n",
    "\n",
    "        tf_embedding_matrix = tf.Variable(initial_value=embedding_matrix,\n",
    "                                          trainable=False, dtype=tf.float32, name='tf_embedding_matrix')\n",
    "        \n",
    "        X_embeddings = tf.nn.embedding_lookup(tf_embedding_matrix, X, name='X_embeddings')\n",
    "        X_embeddings_reshaped = tf.reshape(X_embeddings, shape=(-1, estimated_sent_len, X_embeddings.get_shape().as_list()[-1]))\n",
    "\n",
    "    with tf.variable_scope('Bi-RNN-1', initializer=tf.contrib.layers.xavier_initializer()):\n",
    "        conc_outputs_1 = create_RNN(tf.contrib.rnn.GRUCell, n_neurons = n_neurons_GRU_1,\n",
    "                                  rnn_input = X_embeddings_reshaped, seq_length = tf_sentences_length)\n",
    "    with tf.variable_scope('Attention-1'):\n",
    "        sentence_vectors, _ = create_attention(conc_outputs_1, attention_n_neurons_1)\n",
    "        sentence_vectors_dropped = tf.nn.dropout(sentence_vectors, keep_prob=tf_keep_prob)\n",
    "        sentence_vectors_reshaped = tf.reshape(sentence_vectors_dropped, shape=(-1, estimated_doc_len, sentence_vectors_dropped.get_shape().as_list()[-1]))\n",
    "    \n",
    "        \n",
    "    with tf.variable_scope('Attention-2'):\n",
    "        doc_vectors, normalized_sentence_attentions  = create_attention(sentence_vectors_reshaped, attention_n_neurons_2)\n",
    "    \n",
    "        \n",
    "    with tf.name_scope('Prediction'):\n",
    "        logits = fully_connected(doc_vectors, 1, activation_fn=None)\n",
    "        prob = tf.nn.sigmoid(logits, name='prob')\n",
    "        x_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits, name='x_entropy')\n",
    "        loss = tf.reduce_mean(x_entropy, name ='loss')\n",
    "      \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_highest_validation_accuracy(validation_accuracy):\n",
    "    with open(os.path.join(data_path,'highest_validation_accuracy.txt'),'w') as f:\n",
    "        f.write(str(validation_accuracy[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "highest_validation_accuracy = float(return_file_content(os.path.join(data_path,'highest_validation_accuracy.txt')))\n",
    "\n",
    "X_valid_samples, y_valid_samples = create_training_batches.create_validation_data(num_pos=65, num_neg=65)\n",
    "valid_sentences_length, valid_documents_length = estimate_sentences_and_document_lengths(X_valid_samples, vocab_dict['my_dummy'])\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    X_train_samples, y_train_samples = create_training_batches.create_training_data(num_pos=25, num_neg=40)\n",
    "    sentences_length, documents_length = estimate_sentences_and_document_lengths(X_train_samples, vocab_dict['my_dummy'])\n",
    "    _, np_prob, np_y = sess.run([training_op, prob, y], feed_dict={X:X_train_samples, y:y_train_samples,\n",
    "                                                                   tf_sentences_length:sentences_length,\n",
    "                                                                   tf_documents_length:documents_length,\n",
    "                                                                   tf_keep_prob:0.9})\n",
    "\n",
    "    if i%50 == 1:\n",
    "        np_prob, np_y = sess.run([prob, y],feed_dict={X:X_valid_samples, y:y_valid_samples,\n",
    "                                                      tf_sentences_length:valid_sentences_length,\n",
    "                                                      tf_documents_length:valid_documents_length,\n",
    "                                                      tf_keep_prob:1})\n",
    "\n",
    "        validation_accuracy = sum((np_prob>0.5)==(np_y>0.5))/len(np_y)\n",
    "        print('Validation Accuracy', i, validation_accuracy)\n",
    "        \n",
    "        if validation_accuracy > highest_validation_accuracy:\n",
    "            write_highest_validation_accuracy(validation_accuracy)\n",
    "            save_path = saver.save(sess, os.path.join(data_path,\"consent.ckpt\"))\n",
    "            print('Saved Highest accurate model')\n",
    "            highest_validation_accuracy = validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_normalized_sentence_attentions, np_prob, np_y = sess.run([normalized_sentence_attentions, prob, y],feed_dict={X:X_valid_samples, y:y_valid_samples,tf_sentences_length:valid_sentences_length, tf_documents_length:valid_documents_length,tf_keep_prob:1})\n",
    "attention_scores = np.squeeze(np_normalized_sentence_attentions)\n",
    "accuracy = sum((np_prob>0.5)==(np_y>0.5))/len(np_y)\n",
    "print('Validation Accuracy', i, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_num =  20\n",
    "np_prob[text_num], np_y[text_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sent in X_valid_samples[text_num]:\n",
    "    for word_id in sent:\n",
    "        word = rev_vocab_dict[word_id]\n",
    "        if word!='my_dummy':\n",
    "            print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_sentence_num = np.argmax(attention_scores,1)\n",
    "\n",
    "text_num =  20\n",
    "for word_id in X_valid_samples[text_num][important_sentence_num[text_num]]:\n",
    "    word = rev_vocab_dict[word_id]\n",
    "    if word!='my_dummy':\n",
    "        print(word, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = 'consent'\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = os.path.join(data_path,\"{}/run-{}\".format(root_logdir, now))\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
