{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_RNN(RNN, n_neurons, rnn_input, seq_length):\n",
    "    # RNN : RNN Cell\n",
    "    # n_neurons : output_size\n",
    "    # rnn_input : (batch_size, n_timesteps, n_features)\n",
    "    # conc_outputs : (batch_size, n_time_steps, output_size*2)\n",
    "    fw_cell = RNN(num_units=n_neurons)\n",
    "    bw_cell = RNN(num_units=n_neurons)\n",
    "\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, rnn_input,\n",
    "                                                  sequence_length = seq_length, dtype=tf.float32)\n",
    "    conc_outputs = tf.concat(outputs, 2, name='conc_outputs')\n",
    "    return conc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_attention(conc_outputs, attention_n_neurons):\n",
    "    # conc_outputs : (batch_size, n_time_steps, rnn_output_size)\n",
    "    # attention_n_neurons : attention_network size\n",
    "    # attention_output : (batch_size, rnn_output_size)\n",
    "    # normalized_attention_scores : (batch_size, n_time_steps, 1)\n",
    "    \n",
    "    attention_vector = tf.get_variable(\"attention_vector\", shape=(attention_n_neurons, 1),\n",
    "                                         initializer=tf.random_uniform_initializer(minval=-0.2, maxval=0.2))\n",
    "\n",
    "    conc_outputs_reshaped = tf.reshape(conc_outputs, shape=(-1, conc_outputs.get_shape().as_list()[-1]),\n",
    "                                         name='conc_outputs_reshaped')\n",
    "    \n",
    "    conc_outputs_reshaped_fully_connected = fully_connected(conc_outputs_reshaped, attention_n_neurons,\n",
    "                                                              activation_fn=tf.nn.tanh,\n",
    "                                                            scope='conc_outputs_reshaped_fully_connected')\n",
    "\n",
    "    un_normalized_attention_scores = tf.matmul(conc_outputs_reshaped_fully_connected, attention_vector,\n",
    "                                               name='un_normalized_attention_scores')\n",
    "    \n",
    "    un_normalized_attention_scores_reshaped = tf.reshape(un_normalized_attention_scores,\n",
    "                                                            shape=(-1, conc_outputs.get_shape().as_list()[1]))\n",
    "    \n",
    "    normalized_attention_scores = tf.expand_dims(tf.nn.softmax(un_normalized_attention_scores_reshaped),2)\n",
    "\n",
    "    attentive_conc_outputs = tf.multiply(normalized_attention_scores, conc_outputs)\n",
    "    attention_output  = tf.reduce_sum(attentive_conc_outputs, axis=1)\n",
    "    return attention_output, normalized_attention_scores\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multicourt_pol",
   "language": "python",
   "name": "multicourt_pol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
